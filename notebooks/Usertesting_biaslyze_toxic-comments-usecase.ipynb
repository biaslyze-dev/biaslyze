{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca7febe-a5e3-41c2-aa2b-a7e56db76969",
   "metadata": {},
   "source": [
    "# Usertesting\n",
    "\n",
    "In this notebook you will see how to test a model with our Biaslyze tool in order to inspect it on hints for possible bias. Biaslyze uses counterfactual token fairness scores to evaluate the significance of concepts and attributes sensible to discrimination within the models decisions. \n",
    "To show you how Biaslyze works we use data from a Kaggle challenge and build a model that classifies texts from online comments as toxic or not toxic. \n",
    "The data consists of instances of 226235 online comments. You can get the data on the kaggle site.\n",
    "\n",
    "Data source: [https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038980f-2cc7-4493-a36a-63db3161f8b0",
   "metadata": {},
   "source": [
    "# Installation\n",
    "First install the Biaslyze python package using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece4fbc-147e-4c62-998a-b83b7ce92654",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install biaslyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787848d-30f6-4512-b85d-5d1d280c5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831829c-9a99-48ff-a8aa-63044cce627a",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0361b-9a49-48a8-89ba-79301470a39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/jigsaw-toxic-comment-classification/train.csv\"); df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e805e-0e04-4790-836d-e04d4f794e7d",
   "metadata": {},
   "source": [
    "## Now make the classification problem binary: \n",
    "Apart from the descriptive multi-label toxicity labels, there is another target column with a binary class signifying if a comment text is toxic or non-toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209acd8a-0920-4de4-ad30-60cf950ace52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"target\"] = df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis=1) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360a203-b400-44f3-8d6a-3308005e4ec4",
   "metadata": {},
   "source": [
    "## Train a BoW-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b97479-2da2-44d2-a771-2181af38e4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28d4aa-17ef-4165-b43b-9bb81a3ee6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = make_pipeline(TfidfVectorizer(min_df=10, max_features=30000, stop_words=\"english\"), LogisticRegression(C=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc41891-f3e7-4935-8d3f-54876c388cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.fit(train_df.comment_text, train_df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74846aa-c436-449d-8c21-cbd8ab5b1526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_df.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58f341-ea7d-4374-b8a6-0207e8a5cf31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = accuracy_score(test_df.target, y_pred)\n",
    "print(\"Test accuracy: {:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a5b43-0a33-4648-86c1-f1c2b69335d0",
   "metadata": {},
   "source": [
    "## Evaluate the model for bias\n",
    "\n",
    "Now that we have a model to test, lets evaluate it with the Biaslyze tool and test the sensible concepts for possible bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80a70e-b0cd-44c2-ae4f-cfce7daffbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biaslyze.bias_detectors import CounterfactualBiasDetector\n",
    "\n",
    "bias_detector = CounterfactualBiasDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c9588-8583-4e4a-8d8b-748c9d217ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_detection_results = bias_detector.process(\n",
    "    texts=test_df.comment_text,\n",
    "    labels=test_df.target.tolist(),\n",
    "    predict_func=clf.predict_proba,\n",
    "    concepts_to_consider=[\"religion\", \"gender\"],\n",
    "    max_counterfactual_samples=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0757e-64ec-4799-879b-d598fa01c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_detection_results.visualize_counterfactual_scores(concept=\"gender\", top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cd06e-4a14-46bd-b7d6-b189d893cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_detection_results.visualize_counterfactual_scores(concept=\"religion\", top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03205723-8c7b-48c6-9501-0206b9c2a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_detection_results.visualize_counterfactual_sample_scores(concept=\"gender\", top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff7f72-1b12-4003-bc71-605dff1b7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_detection_results.visualize_counterfactual_scores(concept=\"religion\", top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79cc43-9437-4608-9055-73e9d48cab0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9e537-72cd-4590-b45c-40c22eea349f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz = counterfactual_detection_results.visualize_counterfactual_score_by_sample_histogram()\n",
    "show(viz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
